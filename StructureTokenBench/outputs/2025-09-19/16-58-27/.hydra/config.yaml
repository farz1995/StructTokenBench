model:
  class_name: SequenceClassificationModel
  pretrained_ckpt_path: ''
  ckpt_path: null
  num_labels: 1
  dropout: 0.1
  num_layer: 1
  hidden_size: 512
  num_tokens: null
  d_model: 128
  is_global_or_local: local
  multi_label: false
  regression: false
  use_sequence: false
  sequence_only: false
  add_noise: null
deepspeed_path: src/script/config/deepspeed/32_stage2.json
default_data_dir: /struct_token_bench_release_data/
tokenizer: WrappedESM3Tokenizer
tokenizer_pretrained_ckpt_path: null
tokenizer_ckpt_name: null
max_steps: 10000
experiment_name: bindbio_esm2_lr0.001
run_name: tryout
validate_only: false
test_only: false
save_dir_path: ''
tokenizer_device: cuda
precompute_tokens: true
quantizer_use_linear_project: false
model_encoder_dmodel: 1024
model_encoder_nlayers: 2
model_encoder_vheads: 128
quantizer_codebook_size: 4096
quantizer_codebook_embed_size: 128
model_encoder_dout: 128
trainer:
  accelerator: gpu
  num_nodes: 1
  check_val_every_n_epoch: 1
  default_root_dir: /struct_token_bench_logs/
  enable_checkpointing: true
  enable_model_summary: true
  enable_progress_bar: true
  devices:
  - 0
  gradient_clip_val: 1.0
  limit_train_batches: null
  limit_val_batches: null
  log_every_n_steps: 20
  max_epochs: null
  max_steps: ${..max_steps}
  num_sanity_val_steps: 0
  precision: 32-true
  reload_dataloaders_every_n_epochs: 1
  val_check_interval: 1.0
  accumulate_grad_batches: 1
  deterministic: true
lightning:
  model_module:
    _target_: model_module.PlModel
    py_logger: null
    optimizer_cfg: null
    model_cfg: null
  data_module:
    _target_: data_module.ProteinDataModule
  callbacks:
    checkpoint:
      _target_: pytorch_lightning.callbacks.ModelCheckpoint
      every_n_train_steps: null
      every_n_epochs: 1
      save_top_k: 3
      monitor: validation_f1_score
      mode: max
      filename: '{epoch}-{step}-{validation_f1_score:.2f}'
      save_last: true
      dirpath: null
    lr_monitor:
      _target_: pytorch_lightning.callbacks.LearningRateMonitor
      logging_interval: step
    progress_bar:
      _target_: pytorch_lightning.callbacks.progress.TQDMProgressBar
      refresh_rate: 10
  logger:
    _target_: pytorch_lightning.loggers.TensorBoardLogger
    save_dir: ${...trainer.default_root_dir}
    name: ${...experiment_name}
  strategy:
    _target_: pytorch_lightning.strategies.DeepSpeedStrategy
    config: ${...deepspeed_path}
    remote_device: null
data:
  num_workers: 0
  prefetch_factor: null
  data_path: ${..default_data_dir}/data/functional/local/
  data_name: BioLIP2FunctionDataset
  target_field: binding_label
  filter_length: 600
  truncation_length: 5000
  multi_label: false
  is_global_or_local: local
  pdb_data_dir: /pdb_data/mmcif_files/
  fast_dev_run: false
  use_continuous: false
  use_sequence: false
optimization:
  micro_batch_size: 8
  seed: 1234
  optimizer:
    name: adam
    lr: 0.001
    betas:
    - 0.9
    - 0.98
    eps: 1.0e-08
    adam_w_mode: true
    weight_decay: 0.01
    params: null
  scheduler:
    _target_: util.get_cosine_schedule_with_warmup
    optimizer: null
    min_ratio: 0.1
    plateau_ratio: 0.1
    num_warmup_steps: 200
    num_training_steps: ${...max_steps}
  override:
    mult_factor: 1.0
    add_index: 0
autorestart:
  filter_keywords:
  - last.ckpt
